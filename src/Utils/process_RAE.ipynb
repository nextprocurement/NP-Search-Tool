{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tomando como base el código de Jorge Dueñas Lerín en https://github.com/JorgeDuenasLerin/diccionario-espanol-txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from urllib.parse import quote\n",
    "from urllib.request import Request, urlopen\n",
    "\n",
    "import regex\n",
    "from lxml import etree\n",
    "from tqdm.notebook import trange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data_RAE = Path(\"../../data/RAE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xtree(url, param, UA):\n",
    "    tree = None\n",
    "    attemps = 5\n",
    "    while attemps > 0 and tree == None:\n",
    "        try:\n",
    "            req = Request(url.format(quote(param)), headers={\"User-Agent\": UA})\n",
    "            # print (req.full_url)\n",
    "            # print (start_withs)\n",
    "            webpage = urlopen(req)\n",
    "            htmlparser = etree.HTMLParser()\n",
    "            tree = etree.parse(webpage, htmlparser)\n",
    "        except Exception as e:\n",
    "            attemps -= 1\n",
    "            print(str(e))\n",
    "            time.sleep(0.5)\n",
    "\n",
    "    return tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove_from_title = \"Ir a la entrada \"\n",
    "\"\"\"\n",
    "Usamos title por que el contenido en determinadas situaciones cambia:\n",
    "<a data-cat=\"FETCH\" data-acc=\"LISTA EMPIEZA POR\" data-eti=\"abollado\" title=\"Ir a la entrada abollado, abollada\" href=\"/abollado\">abollado<sup>1</sup>, da</a>\n",
    "\"\"\"\n",
    "skip = len(to_remove_from_title)\n",
    "\n",
    "letras = [\n",
    "    \"a\", \"á\", \"b\", \"c\", \"d\", \"e\", \"é\", \"f\", \"g\", \"h\", \"i\", \"í\", \"j\", \"k\", \"l\", \"m\", \"n\",\n",
    "    \"ñ\", \"o\", \"ó\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"ú\", \"ü\", \"v\", \"w\", \"x\", \"y\", \"z\",\n",
    "]\n",
    "\n",
    "start_withs = letras.copy()\n",
    "\n",
    "UA = \"Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:47.0) Gecko/20100101 Firefox/47.0\"\n",
    "# RAE dict (m=31 means all elements starting with {})\n",
    "url_list = \"https://dle.rae.es/{}?m=31\"\n",
    "url_detail = \"https://dle.rae.es/{}\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buscar cada palabra única que aparece en el diccionario. Estas pueden ser flexiones de género (como \"abadesa\" a partir de \"abad\"), aunque se guardará igualmente. Más tarde serán filtradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial = \"\"\n",
    "# total_palabras = []\n",
    "\n",
    "# while len(start_withs) != 0:\n",
    "#     palabra_start_with = start_withs.pop(0)\n",
    "#     curr = palabra_start_with[0]\n",
    "#     if initial!=curr:\n",
    "#         initial = curr\n",
    "#         print(f\"\\nSearch '{initial}'\")\n",
    "#         if total_palabras:\n",
    "#             with dir_data_RAE.joinpath(\"total_palabras.txt\").open(\"a\", encoding=\"utf-8\") as f:\n",
    "#                 f.writelines([f\"{p}\\n\" for p in set(total_palabras)])\n",
    "#         total_palabras = []\n",
    "\n",
    "#     if(palabra_start_with in ['app', 'docs', 'js']):\n",
    "#         continue\n",
    "\n",
    "#     tree = get_xtree(url_list, palabra_start_with, UA)\n",
    "#     res = tree.xpath('//*[@id=\"resultados\"]/*/div[@class=\"n1\"]/a/@title')\n",
    "\n",
    "#     # Se repiten palabras. Cuando por ejemplo aba tiene más de 30 y se exapande\n",
    "#     # abaa, abab, etc... las primeras palabras no aparecen: aba\n",
    "#     for pal in res:\n",
    "#         pal_clean = pal[skip:]\n",
    "#         pal_clean = pal_clean.split(\", \")\n",
    "#         for pal in res:\n",
    "#             pal_clean = pal[skip:]\n",
    "#             pal_clean = pal_clean.split(\", \")\n",
    "#             total_palabras.extend(pal_clean)\n",
    "\n",
    "#     if(len(res)>30):\n",
    "#         # print(\"!\" * 80)\n",
    "#         # print(\"EXAPEND: \" + palabra_start_with)\n",
    "#         print(f\"p-count: {len(set(total_palabras))}\", end=\"\\r\")\n",
    "#         expand = [palabra_start_with + l for l in letras]\n",
    "#         start_withs = expand + start_withs\n",
    "\n",
    "# if total_palabras:\n",
    "#     with dir_data_RAE.joinpath(\"total_palabras.txt\").open(\"a\", encoding=\"utf-8\") as f:\n",
    "#         f.writelines([f\"{p}\\n\" for p in set(total_palabras)])\n",
    "\n",
    "# # Ensure vocab is unique\n",
    "# with dir_data_RAE.joinpath(\"total_palabras.txt\").open(\"r\", encoding=\"utf-8\") as f:\n",
    "#     vocab = sorted(list(set([p.strip() for p in f.readlines()])))\n",
    "# with dir_data_RAE.joinpath(\"total_palabras.txt\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "#     f.writelines([f\"{p}\\n\" for p in vocab])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos una búsqueda individual de cada uno de los elementos anteriores y filtramos según lo siguiente:\n",
    "- Se eliminará si no tiene significado (p.e. palabra derivada por flexión de género).\n",
    "- Si tiene locuciones o expresiones, estas se mantienen como elementos independientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_loc(res: str, word: str):\n",
    "    \"\"\"\n",
    "    Obtener expresiones, locuciones, etc. de manera única.\n",
    "    Ej: \"hacerse agua, o un agua, en la boca\" -> [\"hacerse agua en la boca\", \"hacerse un agua en la boca\"]\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    res: str\n",
    "        Original expression as it appears in dictionary.\n",
    "    word: str\n",
    "        The term that is used as base for the expression.\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    results: list(str)\n",
    "        List of all possible combinations of the expression.\n",
    "    \"\"\"\n",
    "    # Split loc into first (before main word), last (after last main word) and middle (rest)\n",
    "    # Examples:\n",
    "    #  -> ['hacerse ', ['agua', ' un agua'], ' en la boca']\n",
    "    # \"romper aguas\" -> ['romper ', 'agua', 's']\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Split in the multiple ways that there are to say something\n",
    "    res = regex.split(r\",\", res)\n",
    "\n",
    "    # If there is only one way to say something\n",
    "    if len(res) == 1:\n",
    "        results.append(regex.sub(r\"\\s+\", \" \", res[0]).strip())\n",
    "        return results\n",
    "    else:\n",
    "        # TODO: there might be two full expressions separated (e.g. \"hablar con, o de, misterio, o hacer misterio\")\n",
    "        # Initial approach:\n",
    "        # first_part = regex.search(rf\"(?:.+?(?={word}))\", res)\n",
    "        # last_part = regex.search(rf\"(?<={word}.*?\\b)(?!.*{word})(.*)\", res)\n",
    "        # middle_part = regex.search(rf\"\\b({word}.*)?({word}.*?)\\b\", res)\n",
    "        # Manage this (although not common)\n",
    "\n",
    "        # Assume last element will be common to all options\n",
    "        # print(\"res\", res)\n",
    "        if not regex.search(r\"\\bo\\b|\\bu\\b\", res[-1]):\n",
    "            last_part = res.pop()\n",
    "        elif word in res[-1]:\n",
    "            opt = regex.sub(r\"\\bo\\b|\\bu\\b\", \"\", res.pop()).strip()\n",
    "            results.append(opt)\n",
    "            last_part = \"\"\n",
    "        else:\n",
    "            last_part = \"\"\n",
    "\n",
    "        rest = \",\".join(res)\n",
    "        if (\n",
    "            ((results) or (word in rest))\n",
    "            and (not last_part)\n",
    "            and (not any([word in r for r in res[:-1]]))\n",
    "        ):\n",
    "            rest = regex.sub(r\"((\\bo\\b|\\bu\\b)\\s+(\\bo\\b|\\bu\\b))+\", \" o \", rest)\n",
    "            possibilities = process_loc(rest, word)\n",
    "            results.extend(possibilities)\n",
    "\n",
    "        else:\n",
    "            # Check what are the elements that form options (all that are joint by \" o \")\n",
    "            options = []\n",
    "            for pos in range(1, len(res)):\n",
    "                if regex.search(r\"\\bo\\b|\\bu\\b\", res[pos]):\n",
    "                    if res[pos - 1] not in options:\n",
    "                        options.append(res[pos - 1])\n",
    "                    if res[pos] not in options:\n",
    "                        options.append(res[pos])\n",
    "\n",
    "            if options:\n",
    "                options = \" o \".join(options)\n",
    "                options = regex.sub(\n",
    "                    r\"((\\bo\\b|\\bu\\b)\\s+(\\bo\\b|\\bu\\b))+\", \" o \", \"\".join(options)\n",
    "                )\n",
    "                options = regex.split(r\"\\bo\\b|\\bu\\b\", options)\n",
    "            else:\n",
    "                options = []\n",
    "\n",
    "            # TODO: change this, so that the other way around (last part) is also satisfied\n",
    "            # Set first part up to first \"word\" appearance\n",
    "            first_part = regex.search(rf\".*(?<!{word}.*)(?={word})\", \"\".join(res))\n",
    "            if first_part:\n",
    "                first_part = first_part.group(0)\n",
    "            else:\n",
    "                first_part = \"\"\n",
    "            if (options) and (word in options[0]) and (word in last_part):\n",
    "                first_part = \"\"\n",
    "            else:\n",
    "                options = \" o \".join(res)\n",
    "                options = regex.sub(\n",
    "                    r\"((\\bo\\b|\\bu\\b)\\s+(\\bo\\b|\\bu\\b))+\", \" o \", \"\".join(options)\n",
    "                )\n",
    "                options = regex.split(r\"\\bo\\b|\\bu\\b\", options)\n",
    "            ###\n",
    "\n",
    "            if any(word in o for o in options):\n",
    "                aux_options = []\n",
    "                prefix = \"\"\n",
    "                for o in options:\n",
    "                    if word in o:\n",
    "                        _prefix = regex.search(rf\".*?{word}.*?\\b\", o)\n",
    "                        if _prefix and word not in last_part:\n",
    "                            prefix = _prefix.group(0)\n",
    "                    if prefix:\n",
    "                        aux_options.append(\n",
    "                            prefix + \" \" + regex.sub(rf\"^{prefix}\", \"\", o)\n",
    "                        )\n",
    "                if aux_options:\n",
    "                    options = aux_options\n",
    "\n",
    "            # print(\"first_part: \", first_part)\n",
    "            # print(\"options: \", options)\n",
    "            # print(\"last_part:\", last_part)\n",
    "\n",
    "            # Then do all the possible combinations: ['hacerse agua en la boca', 'hacerse un agua en la boca']\n",
    "            for o in options:\n",
    "                result = (\n",
    "                    first_part\n",
    "                    + regex.sub(rf\"{first_part}|{last_part}\", \"\", o)\n",
    "                    + last_part\n",
    "                )\n",
    "                result = regex.sub(r\"\\s+\", \" \", result).strip()\n",
    "                results.append(result)\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # These expressions might not exists, they are just for testing purposses\n",
    "# word = \"agua\"\n",
    "# expresiones = [\n",
    "#     \"hacerse agua, o un agua, en la boca\",\n",
    "#     \"hacer agua por las cacholas, o por los imbornales\",\n",
    "#     \"echar, o tirar, agua\",\n",
    "#     \"agua mineral, o agua natural\",\n",
    "#     \"como agua, o como el agua, en el pez\",\n",
    "#     \"romper aguas\",\n",
    "#     \"agua del norte, agua falsa, o agua roja\",\n",
    "#     \"bailar el coso del agua, o de las aguas\",\n",
    "#     \"habérsele muerto agua, o no necesitar, o no tener, agua\",\n",
    "#     \"hablar con, o de, agua, o hacer agua\",\n",
    "# ]\n",
    "# print(*[f\"{process_loc(exp, word)}\\n\" for exp in expresiones])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with dir_data_RAE.joinpath(\"vocabulary.json\").open(\"r\", encoding=\"utf-8\") as f:\n",
    "#     vocabulary = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word = \"aconsejado\"\n",
    "# tree = get_xtree(url_detail, word, UA)\n",
    "# definitions = tree.xpath(\n",
    "#     '//*[@id=\"resultados\"]/article/*[(contains(@class,\"j\"))]/*[@class=\"d\"]/text()'\n",
    "# )\n",
    "# reference = tree.xpath('//*[@id=\"resultados\"]/*/div[@class=\"n1\"]/a/@data-eti')\n",
    "# expressions = tree.xpath(\n",
    "#     '//*[@id=\"resultados\"]/article/*[(contains(@class,\"k6\")) or (contains(@class,\"l\"))]'\n",
    "# )\n",
    "\n",
    "# # It is a reference to word with definition (e.g. word in femenine references to same word in masculine)\n",
    "# if not definitions and reference:\n",
    "#     for r in reference:\n",
    "#         ref_tree = get_xtree(url_detail, r, UA)\n",
    "#         ref_definitions = ref_tree.xpath(\n",
    "#             '//*[@id=\"resultados\"]/article/*[(contains(@class,\"j\"))]/*[@class=\"d\"]/text()'\n",
    "#         )\n",
    "#         if ref_definitions:\n",
    "#             if any([d == \"f.\" for d in ref_definitions]):\n",
    "#                 # vocabulary[word] = word\n",
    "#                 print(word, word)\n",
    "#             else:\n",
    "#                 # vocabulary[word] = r\n",
    "#                 print(word, r)\n",
    "#             break\n",
    "\n",
    "# # word has its own meaning\n",
    "# elif definitions:\n",
    "#     if any([d == \"f.\" for d in definitions]):\n",
    "#         # vocabulary[word] = word\n",
    "#         print(word, word)\n",
    "#     else:\n",
    "#         entry = tree.xpath('//*[@id=\"resultados\"]/article/header')\n",
    "#         if entry:\n",
    "#             entry = regex.sub(r\"\\d\", \"\", \"\".join(entry[0].itertext()).split(\",\")[0])\n",
    "#             # vocabulary[word] = entry\n",
    "#             print(word, entry)\n",
    "#         else:\n",
    "#             # vocabulary[word] = word\n",
    "#             print(word, word)\n",
    "#     # vocabulary[word] = word\n",
    "#     # print(word, word)\n",
    "# # word is just a reference to actual meaning\n",
    "# else:\n",
    "#     # vocabulary[word] = word\n",
    "#     print(word, word)\n",
    "# if expressions:\n",
    "#     final_expr = []\n",
    "#     expr = []\n",
    "#     for el in expressions:\n",
    "#         text_parts = el.xpath(\n",
    "#             'text() | span[text()=\"o\"] | span[text()=\"u\"]  | u | em | a'\n",
    "#         )\n",
    "#         aux_expr = []\n",
    "#         for p in text_parts:\n",
    "#             if isinstance(p, str):\n",
    "#                 aux_expr.append(regex.sub(r\"\\s.+\", \"\", p))\n",
    "#             else:\n",
    "#                 aux_expr.append(regex.sub(r\"\\s.+\", \"\", \"\".join(p.itertext())))\n",
    "#         aux_expr = regex.sub(r\"\\s+\", \" \", \"\".join(aux_expr).strip())\n",
    "#         # Exclude \"Véase\"\n",
    "#         if not aux_expr.endswith(\".\"):\n",
    "#             expr.append(aux_expr)\n",
    "#     # expr = [regex.sub(r\"\\s+\", \" \", \"\".join(el.xpath('text() | span[text()=\"o\"]/text() | u/text()')).strip()) for el in expressions]\n",
    "#     [final_expr.extend(process_loc(l, word)) for l in expr]\n",
    "#     # print(final_expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for el in expressions:\n",
    "#     text_parts = el.xpath(\n",
    "#         'text() | span[text()=\"o\"] | span[text()=\"u\"]  | u | em | a'\n",
    "#     )\n",
    "#     aux_expr = []\n",
    "#     for p in text_parts:\n",
    "#         if isinstance(p, str):\n",
    "#             aux_expr.append(p)\n",
    "#             # print(p)\n",
    "#         else:\n",
    "#             aux_expr.append(regex.sub(r\",\\s.+\", \" \", \"\".join(p.itertext())))\n",
    "#             # print(\"\".join(p.itertext()))\n",
    "#     aux_expr = regex.sub(r\"\\s+\", \" \", \"\".join(aux_expr).strip())\n",
    "#     print(aux_expr)\n",
    "#     # print(\"\".join(p), aux_expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with dir_data_RAE.joinpath(\"vocabulary.json\").open(\"r\", encoding=\"utf-8\") as f:\n",
    "    vocabulary = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4543dd3c959345f6ba887fed561ec2ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5255 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with dir_data_RAE.joinpath(\"total_palabras.txt\").open(\"r\", encoding=\"utf-8\") as f:\n",
    "    vocab = sorted(list(set([p.strip() for p in f.readlines()])))\n",
    "\n",
    "initial = \"\"\n",
    "# vocabulary = {}\n",
    "t = trange(104125, len(vocab), 1, desc=\"\", leave=True)\n",
    "\n",
    "for i in t:\n",
    "    word = vocab[i]\n",
    "    curr = word[0]\n",
    "    if initial != curr:\n",
    "        initial = curr\n",
    "        t.set_description(f\"Processing: '{initial}'\")\n",
    "        t.refresh()\n",
    "\n",
    "    if \"-\" not in word:\n",
    "        tree = get_xtree(url_detail, word, UA)\n",
    "        definitions = tree.xpath(\n",
    "            '//*[@id=\"resultados\"]/article/*[(contains(@class,\"j\"))]/*[@class=\"d\"]/text()'\n",
    "        )\n",
    "        reference = tree.xpath('//*[@id=\"resultados\"]/*/div[@class=\"n1\"]/a/@data-eti')\n",
    "        expressions = tree.xpath(\n",
    "            '//*[@id=\"resultados\"]/article/*[(contains(@class,\"k6\")) or (contains(@class,\"l\"))]'\n",
    "        )\n",
    "        vease = tree.xpath(\n",
    "            '//*[@id=\"resultados\"]/article/*[(contains(@class,\"l\")) and (contains(abbr,\"V.\"))]/a/text()'\n",
    "        )\n",
    "        if vease:\n",
    "            vease = [regex.sub(r\"\\.\", \"\", w) for w in vease]\n",
    "        reference = reference + vease\n",
    "\n",
    "        # It is a reference to word with definition (e.g. word in femenine references to same word in masculine)\n",
    "        if not definitions and reference:\n",
    "            for r, _ in Counter(vease + reference).most_common():\n",
    "                ref_tree = get_xtree(url_detail, r, UA)\n",
    "                ref_definitions = ref_tree.xpath(\n",
    "                    '//*[@id=\"resultados\"]/article/*[(contains(@class,\"j\"))]/*[@class=\"d\"]/text()'\n",
    "                )\n",
    "                if ref_definitions:\n",
    "                    if any([d == \"f.\" for d in ref_definitions]):\n",
    "                        vocabulary[word] = word\n",
    "                        # print(word, word)\n",
    "                    else:\n",
    "                        vocabulary[word] = r\n",
    "                        # print(word, r)\n",
    "                    break\n",
    "\n",
    "        # word has its own meaning\n",
    "        elif definitions:\n",
    "            if any([d == \"f.\" for d in definitions]):\n",
    "                vocabulary[word] = word\n",
    "                # print(word, word)\n",
    "            else:\n",
    "                entry = tree.xpath('//*[@id=\"resultados\"]/article/header')\n",
    "                if entry:\n",
    "                    entry = regex.sub(\n",
    "                        r\"\\d\", \"\", \"\".join(entry[0].itertext()).split(\",\")[0]\n",
    "                    )\n",
    "                    vocabulary[word] = entry\n",
    "                    # print(word, entry)\n",
    "                else:\n",
    "                    vocabulary[word] = word\n",
    "                # print(word, entry)\n",
    "            # vocabulary[word] = word\n",
    "            # print(word, word)\n",
    "        # word is just a reference to actual meaning\n",
    "        else:\n",
    "            vocabulary[word] = word\n",
    "            # print(word, word)\n",
    "\n",
    "        # # uses of that word in expressions\n",
    "        # if expressions:\n",
    "        #     final_expr = []\n",
    "        #     expr = []\n",
    "        #     for el in expressions:\n",
    "        #         text_parts = el.xpath(\n",
    "        #             'text() | span[text()=\"o\"] | span[text()=\"u\"]  | u | em | a'\n",
    "        #         )\n",
    "        #         aux_expr = []\n",
    "        #         for p in text_parts:\n",
    "        #             if isinstance(p, str):\n",
    "        #                 aux_expr.append(p)\n",
    "        #             else:\n",
    "        #                 aux_expr.append(regex.sub(r\",\\s.+\", \" \", \"\".join(p.itertext())))\n",
    "        #         aux_expr = regex.sub(r\"\\s+\", \" \", \"\".join(aux_expr).strip())\n",
    "        #         # Exclude \"Véase\"\n",
    "        #         if not aux_expr.endswith(\".\"):\n",
    "        #             expr.append(aux_expr)\n",
    "        #     # expr = [regex.sub(r\"\\s+\", \" \", \"\".join(el.xpath('text() | span[text()=\"o\"]/text() | u/text()')).strip()) for el in expressions]\n",
    "        #     [final_expr.extend(process_loc(l, word)) for l in expr]\n",
    "        #     with dir_data_RAE.joinpath(\"expresiones.txt\").open(\n",
    "        #         \"a\", encoding=\"utf-8\"\n",
    "        #     ) as f:\n",
    "        #         f.writelines([f\"{p}\\n\" for p in set(final_expr)])\n",
    "\n",
    "    # Save vocab every 1000 words\n",
    "    if not i % 1000:\n",
    "        with dir_data_RAE.joinpath(\"vocabulary.json\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(vocabulary, f)\n",
    "with dir_data_RAE.joinpath(\"vocabulary.json\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(vocabulary, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with dir_data_RAE.joinpath(\"vocabulary.json\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(vocabulary, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with dir_data_RAE.joinpath(\"expresiones.txt\").open(\"r\", encoding=\"utf8\") as f:\n",
    "    expresiones = [l.strip() for l in f.readlines()]\n",
    "with dir_data_RAE.joinpath(\"expresiones.txt\").open(\"w\", encoding=\"utf8\") as f:\n",
    "    f.writelines(f\"{l}\\n\" for l in sorted(list(set(expresiones))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
