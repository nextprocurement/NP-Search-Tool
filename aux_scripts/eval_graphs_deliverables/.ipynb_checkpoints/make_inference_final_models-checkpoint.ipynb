{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4bd815c3-e247-47da-b4fa-b293673e0155",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import pathlib\n",
    "import sys\n",
    "from typing import List\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "from subprocess import check_output\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7b3565c7-0d06-444c-aa53-9599fb6e32b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mallet_path = pathlib.Path(\"/export/usuarios_ml4ds/lbartolome/NextProcurement/NP-Search-Tool/sample_data/models/mallet-2.0.8/bin/mallet\")\n",
    "\n",
    "def infer_thetas(path_model, num_topics, docs):\n",
    "    num_iterations = 1000\n",
    "    doc_topic_thr = 0.0\n",
    "    holdout_corpus = path_model / \"infer_data\" / \"corpus.txt\"\n",
    "    with holdout_corpus.open(\"w\", encoding=\"utf8\") as fout:\n",
    "        for i, t in docs:\n",
    "            fout.write(f\"{i} 0 {t}\\n\")\n",
    "    print(f\"-- -- Mallet corpus.txt for inference created.\")\n",
    "\n",
    "    # Get inferencer\n",
    "    inferencer = path_model / \"model_data\" / \"inferencer.mallet\"\n",
    "\n",
    "    # Files to be generated thoruogh Mallet\n",
    "    corpus_mallet_inf = path_model / \"infer_data\" / \"corpus_inf.mallet\"\n",
    "    doc_topics_file = path_model / \"infer_data\" / \"doc-topics-inf.txt\"\n",
    "\n",
    "\n",
    "    # Extract pipe\n",
    "    # Get corpus file\n",
    "    path_corpus = path_model / \"train_data\" / \"corpus.mallet\"\n",
    "    if not path_corpus.is_file():\n",
    "        print(f\"-- Pipe extraction: Could not locate corpus file\")\n",
    "\n",
    "    # Create auxiliary file with only first line from the original corpus file\n",
    "    path_txt = path_model / \"train_data\" / \"corpus.txt\"\n",
    "    with path_txt.open('r', encoding='utf8') as f:\n",
    "        first_line = f.readline()\n",
    "    path_aux = path_model / \"train_data\" / \"corpus_aux.txt\"\n",
    "    with path_aux.open('w', encoding='utf8') as fout:\n",
    "        fout.write(first_line + '\\n')\n",
    "\n",
    "    # We perform the import with the only goal to keep a small file containing the pipe\n",
    "    print(f\"-- Extracting pipeline\")\n",
    "    path_pipe = path_model / \"train_data\" / \"import.pipe\"\n",
    "\n",
    "    cmd = mallet_path.as_posix() + \\\n",
    "        ' import-file --use-pipe-from %s --input %s --output %s'\n",
    "    cmd = cmd % (path_corpus, path_aux, path_pipe)\n",
    "\n",
    "    try:\n",
    "        print(f'-- Running command {cmd}')\n",
    "        check_output(args=cmd, shell=True)\n",
    "    except:\n",
    "        print('-- Failed to extract pipeline. Revise command')\n",
    "\n",
    "    # Import data to mallet\n",
    "    print('-- Inference: Mallet Data Import')\n",
    "\n",
    "    #\n",
    "    cmd = mallet_path.as_posix() + \\\n",
    "        ' import-file --use-pipe-from %s --input %s --output %s'\n",
    "    cmd = cmd % (path_pipe, holdout_corpus, corpus_mallet_inf)\n",
    "\n",
    "    try:\n",
    "        print(f'-- Running command {cmd}')\n",
    "        check_output(args=cmd, shell=True)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('-- Mallet failed to import data. Revise command')\n",
    "\n",
    "    # Get topic proportions\n",
    "    print('-- Inference: Inferring Topic Proportions')\n",
    "\n",
    "    cmd = mallet_path.as_posix() + \\\n",
    "        ' infer-topics --inferencer %s --input %s --output-doc-topics %s ' + \\\n",
    "        ' --doc-topics-threshold ' + str(doc_topic_thr) + \\\n",
    "        ' --num-iterations ' + str(num_iterations)\n",
    "    cmd = cmd % (inferencer, corpus_mallet_inf, doc_topics_file)\n",
    "\n",
    "    try:\n",
    "        print(f'-- Running command {cmd}')\n",
    "        check_output(args=cmd, shell=True)\n",
    "    except:\n",
    "        print('-- Mallet inference failed. Revise command')\n",
    "\n",
    "    cols = [k for k in np.arange(2, num_topics + 2)]\n",
    "    thetas32 = np.loadtxt(doc_topics_file, delimiter='\\t', dtype=np.float32, usecols=cols)\n",
    "    thetas32[thetas32 < 3e-3] = 0\n",
    "    thetas32 = normalize(thetas32, axis=1, norm='l1')\n",
    "    thetas32 = sparse.csr_matrix(thetas32, copy=True)\n",
    "    \n",
    "    path_save = path_model / \"infer_data\" / \"thetas.npz\"\n",
    "    sparse.save_npz(path_save, thetas32)\n",
    "    \n",
    "    return thetas32.shape\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1ff51b53-2503-4c6d-9d55-088b392620f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "################\n",
    "# Paths to data\n",
    "################\n",
    "path_parquets = pathlib.Path(\"/export/usuarios_ml4ds/cggamella/NP-Search-Tool/sample_data/all_processed\")\n",
    "path_place_without_lote = path_parquets / \"minors_insiders_outsiders_origen_sin_lot_info.parquet\"\n",
    "path_place_esp = path_parquets / \"df_esp_langid.parquet\"\n",
    "path_manual_stops = \"/export/usuarios_ml4ds/lbartolome/NextProcurement/NP-Search-Tool/sample_data/stopwords_sin_duplicados\"\n",
    "path_eq = \"/export/usuarios_ml4ds/lbartolome/NextProcurement/NP-Search-Tool/sample_data/eq.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba025c4-3e2d-4008-925f-329184d5c031",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- -- Reading data from /export/usuarios_ml4ds/cggamella/NP-Search-Tool/sample_data/all_processed/df_esp_langid.parquet and /export/usuarios_ml4ds/cggamella/NP-Search-Tool/sample_data/all_processed/minors_insiders_outsiders_origen_sin_lot_info.parquet\n"
     ]
    }
   ],
   "source": [
    "################\n",
    "# Read data\n",
    "################\n",
    "print(f\"-- -- Reading data from {path_place_esp} and {path_place_without_lote}\")\n",
    "processed = pd.read_parquet(path_place_esp)\n",
    "cols = processed.columns.values.tolist()\n",
    "print(f\"-- -- Data read from {path_place_esp}: {len(processed)} rows.\")\n",
    "# set identifier as column so we dont loose it\n",
    "processed['identifier'] = processed.index\n",
    "print(f\"-- -- Columns: {cols}\")\n",
    "place_without_lote = pd.read_parquet(path_place_without_lote)\n",
    "print(f\"-- -- Data read from {path_place_without_lote}: {len(place_without_lote)} rows.\")\n",
    "\n",
    "#########################\n",
    "# Get additional metadata\n",
    "#########################\n",
    "# Merge 'processed' with 'place_without_lote' to get info about the source of the tender (minors, insiders, outsiders)\n",
    "processed = pd.merge(processed, place_without_lote, how='left', on='id_tm')\n",
    "processed.set_index('identifier', inplace=True)  # Preserved index\n",
    "processed = processed[cols + [\"origen\"]]  #  Keep necessary columns\n",
    "print(f\"-- -- Data merged: {len(processed)} rows.\")\n",
    "#print(f\"-- -- Sample: {processed.head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8390acb-3156-4036-a1ba-7336922d1296",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#####################\n",
    "# Filter stops /eqs #\n",
    "#####################\n",
    "# Filter stops\n",
    "stopwords = set()\n",
    "# Lista para registrar los nombres de los archivos procesados\n",
    "archivos_procesados = []\n",
    "# Iterar sobre cada archivo en el directorio especificado\n",
    "for archivo in os.listdir(path_manual_stops):\n",
    "    if archivo.endswith('.txt'):\n",
    "        ruta_completa = os.path.join(path_manual_stops, archivo)\n",
    "        with open(ruta_completa, 'r', encoding='utf-8') as f:\n",
    "            stopwords.update(f.read().splitlines())\n",
    "        # Registrar el archivo procesado\n",
    "        archivos_procesados.append(archivo)\n",
    "print(f\"-- -- There are {len(stopwords)} stopwords\")\n",
    "def eliminar_stopwords(fila):\n",
    "    return ' '.join([palabra for palabra in fila.split() if palabra not in stopwords])\n",
    "start = time.time()\n",
    "processed['lemmas'] = processed['lemmas'].apply(eliminar_stopwords)\n",
    "print(f\"-- -- Stops filtered in {time.time() - start}\")\n",
    "\n",
    "# Filter eqs\n",
    "start = time.time()\n",
    "pares_diccionario = {}\n",
    "compiled_regexes = {}\n",
    "with open(path_eq, 'r') as archivo:\n",
    "    for linea in archivo:\n",
    "        linea = linea.strip()\n",
    "        palabras = linea.split(':')\n",
    "        if len(palabras) < 2:\n",
    "            print(f\"Línea omitida o incompleta: '{linea}'\")\n",
    "            continue\n",
    "        pares_diccionario[palabras[0]] = palabras[1]\n",
    "pares_diccionario = \\\n",
    "    dict(sorted(pares_diccionario.items(), key=lambda x: x[0]))\n",
    "print(f\"-- -- There are {len(pares_diccionario)} equivalences\")\n",
    "print(\"-- -- Eq dict constructed in :\", time.time() - start)\n",
    "\n",
    "def replace_keywords(lst, keyword_dict):\n",
    "    return \" \".join([keyword_dict.get(word, word) for word in lst])\n",
    "\n",
    "start = time.time()\n",
    "processed[\"lemmas_split\"] = processed['lemmas'].apply(lambda x: x.split())\n",
    "processed['lemmas'] = processed['lemmas_split'].apply(\n",
    "    lambda x: replace_keywords(x, pares_diccionario))\n",
    "processed = processed.drop(columns=['lemmas_split'])\n",
    "print(\"-- -- Eq substituted in:\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e271a55-27db-4ab2-8929-06fa57db6c63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pares_diccionario[\"nanociència_i\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bf4c33-606a-44f3-ab93-5ebdaacbe509",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "############################\n",
    "# Filter by lemmas min len #\n",
    "############################\n",
    "min_lemmas = 1\n",
    "min_lemmas_tm = 2\n",
    "processed['len'] = processed['lemmas'].apply(lambda x: len(x.split()))\n",
    "#processed = processed[processed['len'] >= min_lemmas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c8ffbf-a178-431a-8d7a-471e9ee9d913",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all = processed.copy()\n",
    "minors = all[all.origen == \"minors\"]\n",
    "outsiders = all[all.origen == \"outsiders\"]\n",
    "insiders = all[all.origen == \"insiders\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f1f075-4f0c-49e1-981d-65120efedef5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get tenders that were not included in the modeling process\n",
    "all_not_tm = all[(all['len'] <= min_lemmas_tm) & (all['len'] >= min_lemmas)]\n",
    "all_not_tm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b380c697-a42a-4db5-97de-7289684898f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all[all['len'] > min_lemmas_tm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f3ce6b-bbaa-49a9-9e3a-e2438424ebc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "minors_not_tm = minors[(minors['len'] <= min_lemmas_tm) & (minors['len'] >= min_lemmas)]\n",
    "minors_not_tm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6487c8-1974-4740-a8f0-fdbcf6cdff88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "minors[minors['len'] > min_lemmas_tm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00e8ed7-e412-40a5-bcd0-7ab1f9b7a0de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outsiders_not_tm = outsiders[(outsiders['len'] <= min_lemmas_tm) & (outsiders['len'] >= min_lemmas)]\n",
    "outsiders_not_tm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0381fd09-8468-4a9a-a634-8793d0f1ed4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outsiders[outsiders['len'] > min_lemmas_tm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2a8f9e-eca2-4c02-bb2e-d3517eb1b282",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "insiders_not_tm = insiders[(insiders['len'] <= min_lemmas_tm) & (insiders['len'] >= min_lemmas)]\n",
    "insiders_not_tm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802543d0-dff1-4bda-848f-d50b0c7af414",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "insiders[insiders['len'] > min_lemmas_tm]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2104e05-d06c-487f-85f3-890f510ad164",
   "metadata": {},
   "source": [
    "## ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af15b6d-3e3d-405d-b93c-f5fc64459850",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_model = pathlib.Path(\"/export/usuarios_ml4ds/lbartolome/NextProcurement/NP-Search-Tool/sample_data/models/Mallet/es_Mallet_all_55_topics_FINAL\")\n",
    "docs = all_not_tm[[\"id_tm\", \"lemmas\"]].values\n",
    "num_topics = 55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79ac507-26a8-47ba-8c73-4e002f360d65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "thetas = sparse.load_npz((path_model / \"model_data/TMmodel\" / \"thetas.npz\"))\n",
    "thetas.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be65d9d0-e864-4757-a162-6a7c3a588da8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "infer_thetas(path_model, num_topics, docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6255ff-b0d2-46b3-a5b3-6dc611667cd3",
   "metadata": {},
   "source": [
    "## OUTSIDERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0279fe2a-10fc-4fa4-8ba1-8377368b01d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_model = pathlib.Path(\"/export/usuarios_ml4ds/lbartolome/NextProcurement/NP-Search-Tool/sample_data/models/Mallet/es_Mallet_outsiders_30_topics_FINAL\")\n",
    "docs = outsiders_not_tm[[\"id_tm\", \"lemmas\"]].values\n",
    "num_topics = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a232e5-0a39-414e-9a9d-69350dccfbcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "infer_thetas(path_model, num_topics, docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd028475-aa2f-4f2e-a089-5c42a3af91fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## INSIDERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab7d20b-ae4c-45e5-8a18-0a103189a276",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_model = pathlib.Path(\"/export/usuarios_ml4ds/lbartolome/NextProcurement/NP-Search-Tool/sample_data/models/Mallet/es_Mallet_insiders_12_topics_FINAL\")\n",
    "docs = insiders_not_tm[[\"id_tm\", \"lemmas\"]].values\n",
    "num_topics = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6837396b-1cb5-4b17-9367-e9360f70cd1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "infer_thetas(path_model, num_topics, docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d863105d-b7da-4944-b28b-5328b51cac81",
   "metadata": {},
   "source": [
    "## MINORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae19397c-04f6-4c80-82a0-fac84bc3efee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_model = pathlib.Path(\"/export/usuarios_ml4ds/lbartolome/NextProcurement/NP-Search-Tool/sample_data/models/Mallet/es_Mallet_minors_40_topics_FINAL\")\n",
    "docs = minors_not_tm[[\"id_tm\", \"lemmas\"]].values\n",
    "num_topics = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6767c9fb-4b3e-49d4-8edc-cebba2044d4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "infer_thetas(path_model, num_topics, docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
