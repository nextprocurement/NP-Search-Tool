# Logger
dir_logger: /app/data/app.log # Directory for logs
console_log: true
file_log: true

# Preprocess Options
use_dask: false  # Whether to use Dask for distributed computing. Set to 'true' if you have a Dask cluster.
subsample: 300000  # The number of random samples to take from the dataset. If you want to use all data, you can comment out this line.

pipe:  # The steps to be taken in the pipeline. Order matters!
  - merge_data      # Merges data from different sources
  - lang_id         # Identifies the language of the text
  - normalization   # Normalizes the text (lowercase, some punctuation, urls, etc.)
  - preprocess      # Preprocesses the data for further analysis

merge_dfs:  # DataFrames to be merged in the 'merge_data' step.
  - minors
  - insiders
  - outsiders

lang:  # Languages to consider in 'lang_id' step. Can be multiple.
  - es

# Define directories
# Modify these paths to fit your directory structure
# This is the default configuration for docker image
dir_data: /app/data  # Root data directory
dir_text_processed: /app/data/metadata/df_normalized.parquet  # Path to output processed text metadata parquet file
dir_output_models: /app/data/models/  # Path to output topic models
dir_mallet: /app/Mallet/bin/mallet # This path shouldn't be changed when using docker

# List loading options
# Modify these values to choose which vocabulary, stopwords and ngrams to use.
# If "all", every file in the directory will be used.
# If false, the default settings will be used
use_vocabulary: false  # Which vocabulary to use
  # - vocabulary
  # - vocabulary_extended
use_stopwords: all  # Which stopwords to use
  # - administraci√≥n
  # - common_stopwords
  # - municipios
use_ngrams: all  # Which ngrams to use
  # - expresiones_ngrams
  # - ngrams
  # - proposed_ngrams

# Number of topics for the topic model
num_topics: 100
