{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Union\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "from tqdm import trange\n",
    "\n",
    "from src.Preprocessor.LanguageDetector import LanguageDetector\n",
    "from src.Preprocessor.NgramProcessor import replace_ngrams, suggest_ngrams\n",
    "from src.Preprocessor.TextProcessor import TextPreprocessor\n",
    "from src.Preprocessor.utils import merge_data\n",
    "from src.Utils.utils import load_stopwords, load_vocabulary\n",
    "\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(dir_df: Path, lang: Union[str, List[str]] = \"all\"):\n",
    "    df = None\n",
    "\n",
    "    df = pd.read_parquet(dir_df)\n",
    "    # Choose languages\n",
    "    if lang and \"lang\" in df.columns:\n",
    "        if lang == \"all\":\n",
    "            return df\n",
    "        lang = list(lang)\n",
    "        df = df[df[\"lang\"].isin(lang)]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_df(df, dir_df: Path):\n",
    "    df.to_parquet(dir_df, engine=\"pyarrow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options\n",
    "use_dask = False\n",
    "# subsample = 200_000\n",
    "subsample = 0\n",
    "# pipe = [\"merge_data\", \"lang_id\", \"normalization\", \"preprocess\"]\n",
    "# pipe = [\"preprocess\"]\n",
    "pipe = [\"lang_id\", \"normalization\"]\n",
    "merge_dfs = [\"minors\", \"insiders\", \"outsiders\"]\n",
    "lang = [\"es\"]\n",
    "\n",
    "# Define directories\n",
    "dir_data = Path(\"data\")\n",
    "dir_metadata = dir_data.joinpath(\"metadata\")\n",
    "dir_stopwords = dir_data.joinpath(\"stopwords\")\n",
    "dir_ngrams = dir_data.joinpath(\"ngrams/ngrams.txt\")\n",
    "dir_vocabulary = dir_data.joinpath(\"RAE/vocabulary_extended.json\")\n",
    "\n",
    "dir_text_metadata = dir_metadata.joinpath(\"df_text.parquet\")\n",
    "dir_text_processed = dir_metadata.joinpath(\"df_processed_full.parquet\")\n",
    "\n",
    "# Load data\n",
    "stop_words = load_stopwords(dir_stopwords, use_stopwords=\"all\")\n",
    "vocabulary = load_vocabulary(dir_vocabulary)\n",
    "with dir_ngrams.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    ngrams = [el.strip() for el in f.readlines()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(dir_metadata.joinpath(\"df_processed_pd.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"preprocessed_text\"].dropna().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# case df_text is not (re)created\n",
    "if not \"merge_data\" in pipe:\n",
    "    if any([p in [\"lang_id\", \"normalization\", \"preprocess\"] for p in pipe]):\n",
    "        # dir_text_processed already exists and can be used\n",
    "        if dir_text_processed.exists():\n",
    "            df_processed = load_df(dir_text_processed)\n",
    "\n",
    "            if subsample:\n",
    "                if dir_text_metadata.exists():\n",
    "                    df_text = load_df(dir_text_metadata)\n",
    "                    df_processed_ids = random.sample(list(df_text.index), subsample)\n",
    "                else:\n",
    "                    df_processed_ids = df_processed.index\n",
    "            else:\n",
    "                df_processed_ids = df_processed.index\n",
    "\n",
    "        else:\n",
    "            # df_text already exists?\n",
    "            if not dir_text_metadata.exists():\n",
    "                print(\n",
    "                    f\"Error: '{dir_text_metadata}' does not exist. Create it first.\"\n",
    "                )\n",
    "                exit()\n",
    "            else:\n",
    "                df_text = load_df(dir_text_metadata)\n",
    "                if subsample:\n",
    "                    df_processed_ids = random.sample(list(df_text.index), subsample)\n",
    "                    df_processed = df_text.loc[df_text.index.isin(df_processed_ids)]\n",
    "                else:\n",
    "                    df_processed_ids = df_text.index\n",
    "                    df_processed = df_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsample = 500_000\n",
    "# if subsample:\n",
    "#     df_processed_ids = random.sample(list(df_text.index), subsample)\n",
    "#     df_processed = df_text.loc[df_text.index.isin(df_processed_ids)]\n",
    "# else:\n",
    "#     df_processed_ids = df_text.index\n",
    "#     df_processed = df_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Merge multiple dataframes\n",
    "# merge_data(dir_metadata, dir_text_metadata, merge_dfs=merge_dfs)    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Language Identification\n",
    "# lang_detector = LanguageDetector(\n",
    "#     library=\"fasttext\", ft_model=str(Path(\"models/lid.176.bin\").absolute())\n",
    "# )\n",
    "\n",
    "# if use_dask:\n",
    "#     ids = df_processed.index.isin(df_processed_ids)\n",
    "#     aux = dd.from_pandas(df_processed.loc[ids][[\"text\"]], npartitions=100)\n",
    "#     aux[\"lang\"] = aux[\"text\"].apply(\n",
    "#         lang_detector.identify_language, meta=(None, \"object\")\n",
    "#     )\n",
    "#     aux = aux.compute()[\"lang\"]\n",
    "#     df_processed.loc[aux.index, \"lang\"] = aux\n",
    "\n",
    "# else:\n",
    "#     df_processed.loc[df_processed_ids, \"lang\"] = df_processed.loc[\n",
    "#         df_processed_ids, \"text\"\n",
    "#     ].apply(lang_detector.identify_language)\n",
    "\n",
    "# # Save\n",
    "# save_df(df_processed, dir_text_processed)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text normalization\n",
    "preprocessor_normalizer = TextPreprocessor(\n",
    "    methods=[\n",
    "        \"lowercase\",\n",
    "        \"remove_urls\",\n",
    "        (\"clean_text\", {\"min_len\": 1}),\n",
    "        \"convert_ngrams\"\n",
    "    ],\n",
    "    ngrams=ngrams,\n",
    ")\n",
    "\n",
    "if use_dask:\n",
    "    ids = df_processed.index.isin(df_processed_ids)\n",
    "    aux = dd.from_pandas(df_processed.loc[ids][[\"text\"]], npartitions=100)\n",
    "    aux[\"normalized_text\"] = aux[\"text\"].apply(\n",
    "        preprocessor_normalizer.preprocess, meta=(None, \"object\")\n",
    "    )\n",
    "    aux = aux.compute()[\"normalized_text\"]\n",
    "    df_processed.loc[aux.index, \"normalized_text\"] = aux\n",
    "\n",
    "else:\n",
    "    df_processed.loc[\n",
    "        df_processed_ids, \"normalized_text\"\n",
    "    ] = df_processed.loc[df_processed_ids, \"text\"].apply(\n",
    "        preprocessor_normalizer.preprocess\n",
    "    )\n",
    "\n",
    "# Save\n",
    "save_df(df_processed, dir_text_processed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full process text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full process text\n",
    "preprocessor_full = TextPreprocessor(\n",
    "    methods=[\n",
    "        \"lowercase\",\n",
    "        \"remove_urls\",\n",
    "        \"lemmatize_text\",\n",
    "        (\"clean_text\", {\"min_len\": 1}),\n",
    "        \"convert_ngrams\",\n",
    "        (\"clean_text\", {\"min_len\": 2}),\n",
    "        \"remove_stopwords\",\n",
    "        # \"tokenize_text\",\n",
    "    ],\n",
    "    stopwords=stop_words,\n",
    "    vocabulary=vocabulary,\n",
    "    ngrams=ngrams,\n",
    ")\n",
    "\n",
    "# Compute and save iteratively\n",
    "step = 1_000\n",
    "indices = range(len(df_processed_ids))\n",
    "\n",
    "# Skip columns already processed\n",
    "skip = 0\n",
    "if \"preprocessed_text\" in df_processed.columns:\n",
    "    skip = len(df_processed[\"preprocessed_text\"].dropna().index)\n",
    "t = trange(skip, len(df_processed_ids), step, desc=\"\", leave=True)\n",
    "\n",
    "if use_dask:\n",
    "    for i in t:\n",
    "        ids = df_processed.index.isin(df_processed_ids[i : i + step])\n",
    "        aux = dd.from_pandas(df_processed.loc[ids][[\"text\"]], npartitions=100)\n",
    "        aux[\"preprocessed_text\"] = aux[\"text\"].apply(\n",
    "            preprocessor_full.preprocess, meta=(None, \"object\")\n",
    "        )\n",
    "        aux = aux.compute()[\"preprocessed_text\"]\n",
    "        df_processed.loc[aux.index, \"preprocessed_text\"] = aux\n",
    "\n",
    "        # Save\n",
    "        save_df(df_processed, dir_text_processed)\n",
    "        df_processed = load_df(dir_text_processed)\n",
    "\n",
    "else:\n",
    "    for i in t:\n",
    "        # t.set_description(f\"\")\n",
    "        # t.refresh()\n",
    "        ids = df_processed_ids[i : i + step]\n",
    "        df_processed.loc[ids, \"preprocessed_text\"] = df_processed.loc[\n",
    "            ids, \"text\"\n",
    "        ].apply(preprocessor_full.preprocess)\n",
    "\n",
    "        # Save\n",
    "        save_df(df_processed, dir_text_processed)\n",
    "        df_processed = load_df(dir_text_processed)\n",
    "\n",
    "# Save\n",
    "save_df(df_processed, dir_text_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_partition[\"lang\"].value_counts()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Preprocessor.NgramProcessor import suggest_ngrams, replace_ngrams\n",
    "\n",
    "df_processed = load_df(dir_text_processed)\n",
    "corpus = df_processed[df_processed[\"lang\"] == \"es\"][\"normalized_text\"]\n",
    "\n",
    "# Stopwords with spaces\n",
    "stw = set([s.lower() for s in stop_words if \" \" in s])\n",
    "stw = sorted([tuple(s.split()) for s in stw], key=len, reverse=True)\n",
    "max_length_ngrams = len(stw[0])\n",
    "stw = {s:\"-\".join(s) for s in stw}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sug_ngrams_ng = suggest_ngrams(corpus, ngram_size=4, stop_words=stw)\n",
    "corpus_ng = corpus[:400_000].apply(lambda x: replace_ngrams(x, sug_ngrams_ng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/stopwords/ngrams.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines([\" \".join([n for n in ng])+\"\\n\" for ng in sug_ngrams_ng])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/stopwords/ngrams.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    ngrams = {el.strip():el.strip().replace(\" \", \"-\") for el in f.readlines()}\n",
    "ngrams"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
